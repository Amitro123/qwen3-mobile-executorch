{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ü¶• Qwen3-0.6B ‚Üí Phone Deployment with Unsloth\n",
                "\n",
                "**Fine-tune + Export GGUF for Android/iOS in 90 minutes**\n",
                "\n",
                "| | |\n",
                "|---|---|\n",
                "| ü§ñ Model | Qwen/Qwen3-0.6B (600M params) |\n",
                "| ‚ö° Framework | Unsloth (2x faster training) |\n",
                "| üì± Output | GGUF Q4_K_M (~400MB) |\n",
                "| ‚è±Ô∏è Runtime | ~90 min on T4 GPU |\n",
                "| üéØ Target | Pixel 6, iPhone 15, any modern phone |\n",
                "\n",
                "---\n",
                "\n",
                "## üöÄ Quick Start\n",
                "1. Enable **GPU T4 x2**: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n",
                "2. Enable **Internet**: Settings ‚Üí Internet ‚Üí On\n",
                "3. **Run All** cells in order\n",
                "4. Download output GGUF file\n",
                "5. Deploy to phone with llama.cpp or PocketPal AI app\n",
                "\n",
                "> ‚ö†Ô∏è **Run cells in order!** Dependencies are version-pinned to avoid conflicts."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "# Pin versions to avoid Kaggle dependency conflicts\n",
                "!pip install -q --upgrade pip\n",
                "!pip install -q fsspec==2024.9.0 datasets==4.2.0 huggingface_hub>=0.23.0\n",
                "!pip install -q psutil sentencepiece protobuf\n",
                "!pip install -q peft accelerate bitsandbytes trl transformers>=4.45.0\n",
                "\n",
                "# Install Unsloth from GitHub (latest optimizations)\n",
                "!pip install -q \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify installation\n",
                "import torch\n",
                "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
                "print(f\"‚úÖ CUDA: {torch.cuda.is_available()} - {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'N/A'}\")\n",
                "\n",
                "if not torch.cuda.is_available():\n",
                "    raise RuntimeError(\"‚ùå GPU required! Enable: Settings ‚Üí Accelerator ‚Üí GPU T4 x2\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Load Model\n",
                "Load Qwen3-0.6B with Unsloth optimizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# IMPORTANT: Import unsloth FIRST for kernel optimizations\n",
                "import unsloth\n",
                "import psutil  # Required by Unsloth trainer\n",
                "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
                "\n",
                "print(f\"‚úÖ Unsloth: {unsloth.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "max_seq_length = 2048\n",
                "dtype = None  # Auto-detect\n",
                "load_in_4bit = False  # Full precision for best quality\n",
                "\n",
                "# Load model\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=\"Qwen/Qwen3-0.6B\",\n",
                "    max_seq_length=max_seq_length,\n",
                "    dtype=dtype,\n",
                "    load_in_4bit=load_in_4bit,\n",
                ")\n",
                "\n",
                "print(f\"‚úÖ Loaded Qwen3-0.6B: {model.num_parameters()/1e6:.0f}M params\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply LoRA for efficient fine-tuning\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=16,  # LoRA rank\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",  # 60% memory reduction\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LoRA applied - training ~1% of parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Prepare Dataset\n",
                "Using Alpaca instruction-following dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "# Prompt template\n",
                "prompt_template = \"\"\"### Instruction:\n",
                "{instruction}\n",
                "\n",
                "### Input:\n",
                "{input}\n",
                "\n",
                "### Response:\n",
                "{output}\"\"\"\n",
                "\n",
                "# Load dataset\n",
                "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
                "\n",
                "def format_prompt(examples):\n",
                "    texts = []\n",
                "    for i, inp, out in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"]):\n",
                "        text = prompt_template.format(instruction=i, input=inp, output=out)\n",
                "        texts.append(text + tokenizer.eos_token)\n",
                "    return {\"text\": texts}\n",
                "\n",
                "dataset = dataset.map(format_prompt, batched=True, remove_columns=dataset.column_names)\n",
                "print(f\"‚úÖ Dataset: {len(dataset):,} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Train Model\n",
                "~15 minutes for 60 steps demo (increase `max_steps` for better results)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=max_seq_length,\n",
                "    dataset_num_proc=2,\n",
                "    packing=False,\n",
                "    args=TrainingArguments(\n",
                "        per_device_train_batch_size=2,\n",
                "        gradient_accumulation_steps=4,\n",
                "        warmup_steps=5,\n",
                "        max_steps=60,  # ‚ö° Demo: 60 steps. Production: 500-2000\n",
                "        learning_rate=2e-4,\n",
                "        fp16=not is_bfloat16_supported(),\n",
                "        bf16=is_bfloat16_supported(),\n",
                "        logging_steps=10,\n",
                "        optim=\"adamw_8bit\",\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"linear\",\n",
                "        seed=42,\n",
                "        output_dir=\"outputs\",\n",
                "        report_to=\"none\",\n",
                "    ),\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Training started...\")\n",
                "stats = trainer.train()\n",
                "print(f\"\\n‚úÖ Training complete!\")\n",
                "print(f\"   Steps: {stats.global_step}\")\n",
                "print(f\"   Loss: {stats.training_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save LoRA adapters\n",
                "model.save_pretrained(\"lora_model\")\n",
                "tokenizer.save_pretrained(\"lora_model\")\n",
                "print(\"‚úÖ LoRA adapters saved\")\n",
                "\n",
                "# Merge into full model\n",
                "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method=\"merged_16bit\")\n",
                "print(\"‚úÖ Merged model saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Export to GGUF\n",
                "Convert to GGUF Q4_K_M format for mobile deployment (~15 min)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Export to GGUF with Q4_K_M quantization\n",
                "# This creates a ~400MB file optimized for mobile\n",
                "model.save_pretrained_gguf(\n",
                "    \"qwen3_phone\", \n",
                "    tokenizer, \n",
                "    quantization_method=\"q4_k_m\"  # Best quality/size ratio for mobile\n",
                ")\n",
                "\n",
                "print(\"‚úÖ GGUF export complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Test Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Quick inference test\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "prompt = \"\"\"### Instruction:\n",
                "Explain quantum computing in simple terms.\n",
                "\n",
                "### Input:\n",
                "\n",
                "### Response:\n",
                "\"\"\"\n",
                "\n",
                "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
                "outputs = model.generate(**inputs, max_new_tokens=100, use_cache=True, temperature=0.7)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Package for Download"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import shutil\n",
                "\n",
                "# Find the GGUF file\n",
                "gguf_files = [f for f in os.listdir('.') if f.endswith('.gguf')]\n",
                "print(\"üì¶ Generated files:\")\n",
                "for f in gguf_files:\n",
                "    size = os.path.getsize(f) / 1e6\n",
                "    print(f\"   {f} ({size:.1f} MB)\")\n",
                "\n",
                "# Copy tokenizer\n",
                "if os.path.exists(\"merged_model/tokenizer.json\"):\n",
                "    shutil.copy(\"merged_model/tokenizer.json\", \"tokenizer.json\")\n",
                "    print(\"   tokenizer.json\")\n",
                "\n",
                "print(\"\\n‚úÖ Ready to download!\")\n",
                "print(\"   1. Click 'Save Version' (top right)\")\n",
                "print(\"   2. After save, go to Output tab\")\n",
                "print(\"   3. Download .gguf and tokenizer.json\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Deploy to Phone"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
                "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
            },
            "outputs": [],
            "source": [
                "# Print deployment instructions\n",
                "print(\"\"\"\n",
                "üì± PHONE DEPLOYMENT GUIDE\n",
                "========================\n",
                "\n",
                "Option 1: Android Apps (Easiest)\n",
                "--------------------------------\n",
                "‚Ä¢ PocketPal AI (Play Store) - Free, supports GGUF\n",
                "‚Ä¢ MLC Chat (Play Store) - Open source\n",
                "‚Üí Import your .gguf file in the app\n",
                "\n",
                "Option 2: ADB Push (Advanced)\n",
                "-----------------------------\n",
                "adb shell mkdir -p /data/local/tmp/llm\n",
                "adb push Qwen3-0.6B.Q4_K_M.gguf /data/local/tmp/llm/\n",
                "adb push tokenizer.json /data/local/tmp/llm/\n",
                "\n",
                "Option 3: Termux + llama.cpp\n",
                "----------------------------\n",
                "pkg install cmake git\n",
                "git clone https://github.com/ggerganov/llama.cpp\n",
                "cd llama.cpp && make\n",
                "./llama-cli -m /path/to/model.gguf -p \"Hello!\" -n 50\n",
                "\n",
                "Option 4: iOS\n",
                "-------------\n",
                "‚Ä¢ LLM Farm (App Store)\n",
                "‚Ä¢ Use Files app to import .gguf\n",
                "\n",
                "üéâ Enjoy your fine-tuned LLM running locally!\n",
                "\"\"\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üìä Summary\n",
                "\n",
                "| Step | Time | Output |\n",
                "|------|------|--------|\n",
                "| Install deps | 3 min | - |\n",
                "| Load model | 2 min | 600M params |\n",
                "| Train (60 steps) | 15 min | LoRA adapters |\n",
                "| Merge model | 1 min | merged_model/ |\n",
                "| GGUF export | 15 min | ~400MB .gguf |\n",
                "| **Total** | **~35 min** | **Phone-ready model** |\n",
                "\n",
                "## üîó Resources\n",
                "- [Unsloth GitHub](https://github.com/unslothai/unsloth)\n",
                "- [llama.cpp](https://github.com/ggerganov/llama.cpp)\n",
                "- [PocketPal AI](https://play.google.com/store/apps/details?id=com.pocketpalai)\n",
                "- [Qwen3 Models](https://huggingface.co/Qwen)\n",
                "\n",
                "---\n",
                "*Created with Unsloth + Kaggle T4 GPU*"
            ]
        }
    ],
    "metadata": {
        "kaggle": {
            "accelerator": "nvidiaTeslaT4",
            "isGpuEnabled": true,
            "isInternetEnabled": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}